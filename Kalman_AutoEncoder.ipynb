{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhJMkk9IbzV7",
        "outputId": "30f09f60-16cc-437f-cb3a-c4044886dafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "\n",
        "def extract_sentences_from_pdf(pdf_path, start_page, end_page):\n",
        "    \"\"\"\n",
        "    Extract sentences from a PDF file between specified pages.\n",
        "\n",
        "    Parameters:\n",
        "    pdf_path (str): The path to the PDF file.\n",
        "    start_page (int): The starting page number (1-indexed).\n",
        "    end_page (int): The ending page number (1-indexed).\n",
        "\n",
        "    Returns:\n",
        "    list: A list of sentences extracted from the specified pages.\n",
        "    \"\"\"\n",
        "    Train_TEXT = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        # Loop through the specified page range (adjust for 0-indexing)\n",
        "        for page_num in range(start_page - 1, end_page):  # Convert to 0-indexed\n",
        "            page = pdf.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            if text:  # Check if text extraction was successful\n",
        "                # Clean the text\n",
        "                text = text.strip()  # Remove leading and trailing whitespace\n",
        "                text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "                text = text.replace('\\t', ' ')  # Replace tabs with a space\n",
        "\n",
        "                # Split the text into sentences\n",
        "                sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "                Train_TEXT.extend(sentences)  # Add sentences to the list\n",
        "\n",
        "    return Train_TEXT\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"Conversation_Boy_Girl.pdf\"\n",
        "Sentence_List = extract_sentences_from_pdf(pdf_path, 1, 25)"
      ],
      "metadata": {
        "id": "aMysuu8ScW72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Sentence_List))\n",
        "\n",
        "for i in range(30):\n",
        "    print(Sentence_List[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCgqRBV8cbrh",
        "outputId": "03ae49e9-333f-413a-c95f-97e62db40894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "457\n",
            "Boy: It's been a while since we talk last time.\n",
            "Girl: It has ....\n",
            "but why?\n",
            "Boy: are you in love again?\n",
            "Girl: No, I haven't ......\n",
            "I don't want to.\n",
            "Boy: I get it, but why haven't you?\n",
            "Girl: It's just hard.\n",
            "I just wish ...\n",
            "Boy: I wish that too Girl: It's been so long.\n",
            "Boy: We don't have to lose time anymore.\n",
            "Girl: I'm afraid, what if we don't.\n",
            "Boy: What if we do?\n",
            "Girl: What can we do then?\n",
            "Boy: We have to be patient, things are becoming.\n",
            "Girl: Things are becoming true again.\n",
            "Boy: I didn't wish that to happen, I'm sorry.\n",
            "Girl: I'm sorry too.\n",
            "I planned my life to be with you not without you and things seemed to be shattered.\n",
            "Boy: My heart is shattered, but I haven't lost hope on us, I never stopped.\n",
            "Girl: I'd never stopped either.\n",
            "I think of you every once in a while, I think if you where could we be right now, if you were the one for me.\n",
            "If we were meant to be.\n",
            "Boy: It's really hard seeing you not by my side, like we swear we would, and we both know I'm, we both know we are.\n",
            "Girl: Now will never know.\n",
            "Boy: We know, but.\n",
            "Girl: But ...\n",
            "Boy: but what if we know, why we don't.\n",
            "Girl: Because I believe it's too complicated, but we can.\n",
            "Just not right now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "rcHokzn9ccXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "max_vocab_size = 20000\n",
        "sequence_length = 100\n",
        "embedding_dim = 400\n",
        "latent_dim = 264\n",
        "batch_size = 32\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "mJ_qW2zpchFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary from file\n",
        "def build_vocab(Sentence_List, max_vocab_size):\n",
        "    word_counter = Counter()\n",
        "    # with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    #     for line in f:\n",
        "    #   tokens = tf.keras.preprocessing.text.text_to_word_sequence(line)\n",
        "    #         word_counter.update(tokens)\n",
        "    for line in Sentence_List:\n",
        "        tokens = tf.keras.preprocessing.text.text_to_word_sequence(line)\n",
        "        word_counter.update(tokens)\n",
        "    most_common_words = [word for word, count in word_counter.most_common(max_vocab_size - 2)]\n",
        "    word_index = {word: index + 2 for index, word in enumerate(most_common_words)}\n",
        "    word_index['<PAD>'] = 0\n",
        "    word_index['<OOV>'] = 1\n",
        "    return word_index"
      ],
      "metadata": {
        "id": "jYGwCyucciSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = build_vocab(Sentence_List, max_vocab_size)\n",
        "vocab_size = len(word_index)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "-ODzAYT8clPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ddb23b-10bc-4bb8-9eea-b1bf97425346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_index\n",
        "for i, (key, value) in enumerate(word_index.items()):\n",
        "    if i == 30:\n",
        "        break\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo-Qe_o038VB",
        "outputId": "333ee12d-fd14-4f03-8cb2-2d301cb1b403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you: 2\n",
            "i: 3\n",
            "boy: 4\n",
            "girl: 5\n",
            "to: 6\n",
            "me: 7\n",
            "we: 8\n",
            "are: 9\n",
            "my: 10\n",
            "i'm: 11\n",
            "love: 12\n",
            "that: 13\n",
            "be: 14\n",
            "with: 15\n",
            "and: 16\n",
            "the: 17\n",
            "it: 18\n",
            "is: 19\n",
            "in: 20\n",
            "really: 21\n",
            "this: 22\n",
            "a: 23\n",
            "don't: 24\n",
            "know: 25\n",
            "of: 26\n",
            "so: 27\n",
            "for: 28\n",
            "never: 29\n",
            "too: 30\n",
            "if: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generator for training\n",
        "def data_generator(Sentence_List, word_index, sequence_length, batch_size):\n",
        "    while True:\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        for line in Sentence_List:\n",
        "          tokens = tf.keras.preprocessing.text.text_to_word_sequence(line)\n",
        "          seq = [word_index.get(word, word_index['<OOV>']) for word in tokens]\n",
        "          seq = pad_sequences([seq], maxlen=sequence_length, padding='post', truncating='post')[0]\n",
        "          X_batch.append(seq)\n",
        "          y_batch.append(seq)\n",
        "          if len(X_batch) == batch_size:\n",
        "              X_batch = np.array(X_batch)\n",
        "              y_batch = np.array(y_batch)\n",
        "              yield (X_batch, y_batch)\n",
        "              X_batch = []\n",
        "              y_batch = []"
      ],
      "metadata": {
        "id": "Rhtn3KIecoyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate steps per epoch\n",
        "num_lines = len(Sentence_List)\n",
        "steps_per_epoch = num_lines // batch_size"
      ],
      "metadata": {
        "id": "zH-P7EjyiddA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the autoencoder model\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length, mask_zero=True)(inputs)\n",
        "x = LSTM(latent_dim, return_sequences=True)(x)\n",
        "outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "blj-ZiC4cpvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02953bb7-00e6-462c-ba7d-1767c83efbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "t1 = time.time()\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "# Manual training loop with train_on_batch\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Get the batch data\n",
        "        X_batch, y_batch = next(data_generator(Sentence_List , word_index, sequence_length, batch_size))\n",
        "\n",
        "        # Perform a training step (forward pass and backpropagation)\n",
        "        loss = autoencoder.train_on_batch(X_batch, y_batch)\n",
        "        total_loss += loss\n",
        "\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}]: Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Save the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0 :\n",
        "        # model_save_path = f\"autoencoder_epoch_{epoch + 1}.keras\"\n",
        "        model_save_path = \"autoencoder_final.keras\"\n",
        "        autoencoder.save(model_save_path)\n",
        "        print(f\"Model saved at: {model_save_path}\")\n",
        "\n",
        "t2  = time.time()\n",
        "print(\"Time Taken for training: \",(t2-t1)/60,\"Minutes\")\n",
        "print(\"Time Taken for training: \",((t2-t1)/60)/60,\"Hours\")\n",
        "\n",
        "# Save the model after training\n",
        "final_model_path = \"autoencoder_final.keras\"\n",
        "autoencoder.save(final_model_path)\n",
        "print(f\"Model saved after training at: {final_model_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "jd7hxY-pcvxe",
        "outputId": "0550c3ed-1ba2-483e-9c8c-db97547545e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8adab79f54f7>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Perform a training step (forward pass and backpropagation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Index-to-word mapping for decoding sequences\n",
        "index_to_word = {index: word for word, index in word_index.items()}\n"
      ],
      "metadata": {
        "id": "BSsUqHQrdAYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_sequence(sequence, max_oov_repeat=3):\n",
        "    decoded = []\n",
        "    oov_count = 0\n",
        "    for idx in sequence:\n",
        "        word = index_to_word.get(idx, '<OOV>')\n",
        "        if word == '<PAD>':  # Skip the padding token\n",
        "            continue\n",
        "        if word == '<OOV>':\n",
        "            oov_count += 1\n",
        "        else:\n",
        "            oov_count = 0  # Reset when a valid word is found\n",
        "\n",
        "        if oov_count <= max_oov_repeat:\n",
        "            decoded.append(word)\n",
        "        else:\n",
        "            break  # Stop decoding if there are too many consecutive <OOV> tokens\n",
        "\n",
        "    return ' '.join(decoded)\n"
      ],
      "metadata": {
        "id": "HKu8DCBAdDDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature-based sampling for diversity in predictions\n",
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "D8-VojTidH2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to reconstruct sentence using temperature sampling\n",
        "def test_sentence_reconstruction(sentence, word_index, sequence_length, temperature=1.0):\n",
        "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(sentence)\n",
        "    seq = [word_index.get(word, word_index['<OOV>']) for word in tokens]\n",
        "    padded_seq = pad_sequences([seq], maxlen=sequence_length, padding='post', truncating='post')[0]\n",
        "    padded_seq = np.array([padded_seq])\n",
        "\n",
        "\n",
        "\n",
        "    # Load the saved model\n",
        "    saved_model_path = \"autoencoder_final.keras\"\n",
        "    loaded_autoencoder = load_model(saved_model_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Get the reconstructed sequence from the model\n",
        "    reconstructed_seq = loaded_autoencoder.predict(padded_seq)\n",
        "\n",
        "    # Decode the original sentence\n",
        "    original_text = sentence\n",
        "    # Decode the reconstructed sentence using temperature sampling\n",
        "    reconstructed_text = decode_sequence(\n",
        "        [sample_with_temperature(reconstructed_seq[0, i, :], temperature=temperature)\n",
        "         for i in range(reconstructed_seq.shape[1])]\n",
        "    )\n",
        "\n",
        "    # Output the results\n",
        "    print(\"Original      : \",original_text)\n",
        "    print(\"Reconstructed : \",reconstructed_text)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "cyLXFOW-dKqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test any sentence form the book, here we took only 5 sentences\n",
        "for i in range(200 , 205):\n",
        "  test_sentence = Sentence_List[i]\n",
        "  test_sentence_reconstruction(test_sentence, word_index, sequence_length, temperature=0.8)\n",
        "test_sentence = \"Boy: You don't have to say something it's okay.\"\n",
        "test_sentence_reconstruction(test_sentence, word_index, sequence_length, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4cPdEXBdW_j",
        "outputId": "2a426463-c257-4895-bd3a-edf12380efd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
            "Original      :  Girl: I'd never felt this for someone.\n",
            "Reconstructed :  girl i'd never stopped either for tell either either either either either either either either either tell either hope why stopped on either either tell either either is either by either either either either either either either know either stopped either either stopped either either either either stopped either either us again me either me either either either either either stopped either either either tell stopped again either either us either either stopped stopped me either either me stopped either either either either stopped either side either either either either either either stopped us stopped stopped either stopped either either\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
            "Original      :  You make me feel so alive.\n",
            "Reconstructed :  you not me right you again shattered me shattered me shattered shattered without be shattered not shattered without for me shattered shattered shattered shattered shattered shattered tell me could and me me again shattered me shattered me without shattered on shattered shattered shattered love me shattered me you without and us me shattered for shattered shattered me shattered not haven't us me shattered without me on without shattered me again me me me me one shattered me me me with me me one happen not me where without since not without only shattered by me me shattered side not and\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "Original      :  So brave, so in love.\n",
            "Reconstructed :  hope again not in love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love a love love love love love love love love love love love love love love love love love love love love love love love love love love love love love by love love love love love love love love love love love love love love love love love love love love love love love love love love love\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
            "Original      :  Boy: I'll keep you safe; I'll be everything and more for you.\n",
            "Reconstructed :  boy i'll wait you tell me be to and now for you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
            "\n",
            "Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7826664953f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
            "Original      :  Girl: You already are.\n",
            "Reconstructed :  girl you not are are are are are are are are are are are are are the are are are are you are been are are are again are the are are are are are are are are again are are are a shattered are are are are the are are are are are are are are are are are the are are are are are are are are you the are are sorry are the are are you are you for are for are are are again you are are are true are the are are are are are\n",
            "\n",
            "Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x782666534820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "Original      :  Boy: You don't have to say something it's okay.\n",
            "Reconstructed :  boy you don't to to happen shattered it's anymore are things are are things are are becoming things things to are anymore since becoming that be are are since to are anymore are never anymore are are becoming anymore anymore things true anymore are are are are shattered are are since anymore not becoming are are are either be either are shattered side things our becoming things are are are anymore are are are time are things are are are are are anymore becoming know things shattered time are anymore things anymore anymore anymore time are either are are things\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CykJpRRtI86A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filter\n"
      ],
      "metadata": {
        "id": "oJcS3fStg2B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pykalman\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEyWF0BvXNeZ",
        "outputId": "549e52cd-1b25-4a34-dfcb-e3a3bf0266ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykalman\n",
            "  Downloading pykalman-0.9.7-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pykalman) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pykalman) (1.13.1)\n",
            "Downloading pykalman-0.9.7-py2.py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykalman\n",
            "Successfully installed pykalman-0.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExO-ay_NbCrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable()  # Register the layer for serialization\n",
        "class KalmanFilterLayer(Layer):\n",
        "    def __init__(self, embedding_dim, **kwargs):\n",
        "        super(KalmanFilterLayer, self).__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize Kalman filter parameters as trainable weights\n",
        "        self.transition_matrix = self.add_weight(\n",
        "            name='transition_matrix',\n",
        "            shape=(self.embedding_dim, self.embedding_dim),\n",
        "            initializer='identity',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.observation_matrix = self.add_weight(\n",
        "            name='observation_matrix',\n",
        "            shape=(self.embedding_dim, self.embedding_dim),\n",
        "            initializer='identity',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.transition_covariance = self.add_weight(\n",
        "            name='transition_covariance',\n",
        "            shape=(self.embedding_dim, self.embedding_dim),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.observation_covariance = self.add_weight(\n",
        "            name='observation_covariance',\n",
        "            shape=(self.embedding_dim, self.embedding_dim),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        super(KalmanFilterLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "        # Simple Kalman smoothing using TensorFlow operations\n",
        "        def kalman_smooth_single_sequence(sequence):\n",
        "            smoothed_sequence = tf.identity(sequence)\n",
        "            return smoothed_sequence\n",
        "\n",
        "        # Apply smoothing to each sequence in the batch\n",
        "        smoothed_batch = tf.map_fn(kalman_smooth_single_sequence, inputs)\n",
        "\n",
        "        return smoothed_batch\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ],
      "metadata": {
        "id": "XiSNiJc6ZDOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJFXEnZSkMEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate steps per epoch\n",
        "num_lines = len(Sentence_List)\n",
        "steps_per_epoch = num_lines // batch_size"
      ],
      "metadata": {
        "id": "GzokK4nObF7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = Input(shape=(sequence_length,))\n",
        "# embd = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length, mask_zero=True)(inputs)\n",
        "# smoothed_embeddings = KalmanFilterLayer(embedding_dim)(embd)\n",
        "# print(embd)\n",
        "# print(smoothed_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqBjyv68rW0Q",
        "outputId": "cfe94604-05c7-47ba-80f1-5ff3f0cbdecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<KerasTensor shape=(None, 100, 400), dtype=float32, sparse=False, name=keras_tensor_226>\n",
            "<KerasTensor shape=(None, 100, 400), dtype=float32, sparse=False, name=keras_tensor_228>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'kalman_filter_layer_10' (of type KalmanFilterLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the autoencoder model\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embd = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length, mask_zero=True)(inputs)\n",
        "smoothed_embeddings = KalmanFilterLayer(embedding_dim)(embd)\n",
        "x = LSTM(latent_dim, return_sequences=True)(smoothed_embeddings)\n",
        "\n",
        "\n",
        "outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "autoencoder_Kalman = Model(inputs, outputs)\n",
        "autoencoder_Kalman.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjuh2kW9ZQEu",
        "outputId": "76d5f199-8d35-427f-f79c-9e15a3a0e578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'kalman_filter_layer' (of type KalmanFilterLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t1 = time.time()\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "# Manual training loop with train_on_batch\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Get the batch data\n",
        "        X_batch, y_batch = next(data_generator(Sentence_List , word_index, sequence_length, batch_size))\n",
        "\n",
        "        # Perform a training step (forward pass and backpropagation)\n",
        "        loss = autoencoder_Kalman.train_on_batch(X_batch, y_batch)\n",
        "        total_loss += loss\n",
        "\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}]: Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 2 == 0 :\n",
        "        # model_save_path = f\"autoencoder_epoch_{epoch + 1}.keras\"\n",
        "        model_save_path = \"autoencoder_Kalman.keras\"\n",
        "        autoencoder_Kalman.save(model_save_path)\n",
        "        print(f\"Model saved at: {model_save_path}\")\n",
        "\n",
        "t2  = time.time()\n",
        "print(\"Time Taken for training: \",(t2-t1)/60,\"Minutes\")\n",
        "print(\"Time Taken for training: \",((t2-t1)/60)/60,\"Hours\")\n",
        "\n",
        "# Save the model after training\n",
        "final_model_path = \"autoencoder_final.keras\"\n",
        "autoencoder_Kalman.save(final_model_path)\n",
        "print(f\"Model saved after training at: {final_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anr5ztCRfhv6",
        "outputId": "5b2efc67-354b-4cfc-a908-2bb01ff68482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape: (32, 100, 400)\n",
            "Smoothed States shape: (100, 32, 400)\n",
            "Inputs shape: (32, 100, 400)\n",
            "Smoothed States shape: (100, 32, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Index-to-word mapping for decoding sequences\n",
        "index_to_word = {index: word for word, index in word_index.items()}"
      ],
      "metadata": {
        "id": "PqIS-3U3gjUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_sequence(sequence, max_oov_repeat=3):\n",
        "    decoded = []\n",
        "    oov_count = 0\n",
        "    for idx in sequence:\n",
        "        word = index_to_word.get(idx, '<OOV>')\n",
        "        if word == '<PAD>':  # Skip the padding token\n",
        "            continue\n",
        "        if word == '<OOV>':\n",
        "            oov_count += 1\n",
        "        else:\n",
        "            oov_count = 0  # Reset when a valid word is found\n",
        "\n",
        "        if oov_count <= max_oov_repeat:\n",
        "            decoded.append(word)\n",
        "        else:\n",
        "            break  # Stop decoding if there are too many consecutive <OOV> tokens\n",
        "\n",
        "    return ' '.join(decoded)\n"
      ],
      "metadata": {
        "id": "6OIyPcvbhLHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature-based sampling for diversity in predictions\n",
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "4fvtrssOhPJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to reconstruct sentence using temperature sampling\n",
        "def test_sentence_reconstruction_Kalman(sentence, word_index, sequence_length, temperature=1.0):\n",
        "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(sentence)\n",
        "    seq = [word_index.get(word, word_index['<OOV>']) for word in tokens]\n",
        "    padded_seq = pad_sequences([seq], maxlen=sequence_length, padding='post', truncating='post')[0]\n",
        "    padded_seq = np.array([padded_seq])\n",
        "\n",
        "\n",
        "\n",
        "    # Load the saved model\n",
        "    saved_model_path = \"autoencoder_Kalman.keras\"\n",
        "    loaded_autoencoder = load_model(saved_model_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Get the reconstructed sequence from the model\n",
        "    reconstructed_seq = loaded_autoencoder.predict(padded_seq)\n",
        "\n",
        "    # Decode the original sentence\n",
        "    original_text = sentence\n",
        "    # Decode the reconstructed sentence using temperature sampling\n",
        "    reconstructed_text = decode_sequence(\n",
        "        [sample_with_temperature(reconstructed_seq[0, i, :], temperature=temperature)\n",
        "         for i in range(reconstructed_seq.shape[1])]\n",
        "    )\n",
        "\n",
        "    # Output the results\n",
        "    print(\"Original      : \",original_text)\n",
        "    print(\"Reconstructed : \",reconstructed_text)\n",
        "    print()"
      ],
      "metadata": {
        "id": "uyz8u_HdhVq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test any sentence form the book, here we took only 5 sentences\n",
        "for i in range(120,125):\n",
        "  test_sentence = Sentence_List[i]\n",
        "  test_sentence_reconstruction_Kalman(test_sentence, word_index, sequence_length, temperature=0.8)\n",
        "# test_sentence = \"Are you craz\"\n",
        "# test_sentence_reconstruction(test_sentence, word_index, sequence_length, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpInDA06hXQK",
        "outputId": "7c2389a9-ba8f-4549-a04f-0f2faffd500f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'kalman_filter_layer_2' (of type KalmanFilterLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
            "Original      :  Boy: (silence).\n",
            "Reconstructed :  boy i'll\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "Original      :  Girl: (crying) Why if you love me so much, why did you broke my heart?\n",
            "Reconstructed :  girl time why if you love me again me again me you long not by\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
            "Original      :  Boy: I won't make any excuses.\n",
            "Reconstructed :  boy i really that happen happen\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
            "Original      :  I got lost I did it all wrong.\n",
            "Reconstructed :  i think planned i didn't it one shattered\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
            "Original      :  Girl: (still crying) I'm really in love with you.\n",
            "Reconstructed :  girl time hard i'm hard seeing love again you\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(120 , 125):\n",
        "  test_sentence = Sentence_List[i]\n",
        "  test_sentence_reconstruction(test_sentence, word_index, sequence_length, temperature=0.8)"
      ],
      "metadata": {
        "id": "5ivdUKeNLo9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d623dc5-7d37-4c23-e890-8f53eefb70c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
            "Original      :  Boy: (silence).\n",
            "Reconstructed :  boy i'll i'll i'll heart i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll are i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll i'll heart i'll i'll i'll\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
            "Original      :  Girl: (crying) Why if you love me so much, why did you broke my heart?\n",
            "Reconstructed :  girl because why if you tell me you me why were you be my would are shattered swear shattered tell me be be me both me side me seemed me me swear both be shattered side shattered me be me seemed seemed me life one one me me were seemed be me me shattered me seemed seemed me seemed were seemed life me know me be me for swear me know side you one swear shattered one haven't shattered swear me be one shattered me swear swear swear one me are without be me me me be be shattered side\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "Original      :  Boy: I won't make any excuses.\n",
            "Reconstructed :  boy i didn't to that haven't happen happen haven't that seeing a haven't that with happen while true happen with happen happen so that haven't long happen happen happen to to that haven't to happen happen happen happen with happen with a seeing once haven't happen to complicated while but that complicated a you to a but once to happen while happen complicated a happen is too a happen happen happen only haven't with a that that happen a happen with that but that while happen a girl haven't too that happen that that to to only happen seeing to\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "Original      :  I got lost I did it all wrong.\n",
            "Reconstructed :  i planned of i think it happen haven't happen once haven't not haven't haven't that once us with lost haven't haven't once while once so once happen haven't happen too happen is a we once not once that while while once a haven't could things but you is happen once with you haven't life while haven't once that too to once is lost haven't haven't on with life haven't once too haven't not us a life once haven't didn't a haven't while us happen happen but us once you but too a happen happen but to haven't a haven't happen\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
            "Original      :  Girl: (still crying) I'm really in love with you.\n",
            "Reconstructed :  girl because has we afraid true love again you you you you you you you you you you you you you you you you you you you you you again again you you you you again you you you you you you you again you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnlO49fUj1aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoDQdS8wj1Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "inFTcoLMj1T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cz1tBlMxj1RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zObTBWk3j1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJ-QYKNWj1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMGFTxdEj1Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NVUGComUj1G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I think of you every once in a while, I think if you where could we be right now, if you were the one for me.\"\n",
        "test_sentence_reconstruction(test_sentence, word_index, sequence_length, temperature=0.8)\n",
        "test_sentence_reconstruction_Kalman(test_sentence, word_index, sequence_length, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTFUXdKLj1EM",
        "outputId": "99d4907b-c335-403f-beef-634747ac4be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "Original      :  I think of you every once in a while, I think if you where could we be right now, if you were the one for me.\n",
            "Reconstructed :  i think of you every once in a while i think if you where could we be right now if you were the one for me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me\n",
            "\n",
            "Model loaded successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
            "Original      :  I think of you every once in a while, I think if you where could we be right now, if you were the one for me.\n",
            "Reconstructed :  i think of you every once in a while i think if you where could we be right now if you were the one for me\n",
            "\n"
          ]
        }
      ]
    }
  ]
}